{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter tweets analysis with Spark\n",
    "\n",
    "Using the twitter Python library, this notebook will go through pulling tweets directly from Twitter containing the keyword 'Bachelorette'. A simple word map-reduction is done on the tweets to see just what the Bachelorette is all about..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitter import Api\n",
    "ConsumerSecretApiSecret = 'myKey'\n",
    "ConsumerKeyApiKey = 'myKey'\n",
    "AccessTokenSecret = 'myKey'\n",
    "AccessToken = 'myKey'\n",
    "\n",
    "api = Api(ConsumerKeyApiKey,ConsumerSecretApiSecret,AccessToken,AccessTokenSecret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter, re, json\n",
    "\n",
    "t = twitter.Api(access_token_key=AccessToken, access_token_secret=AccessTokenSecret, consumer_key=ConsumerKeyApiKey, consumer_secret=ConsumerSecretApiSecret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing tweets based on terms (no location)\n",
    "\n",
    "Use the twitter api object's GetStreamFilter to retrieve the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** 50 tweets captured ********\n",
      "******** 100 tweets captured ********\n",
      "******** 150 tweets captured ********\n",
      "******** 200 tweets captured ********\n",
      "******** 250 tweets captured ********\n",
      "******** 300 tweets captured ********\n",
      "******** 350 tweets captured ********\n",
      "******** 400 tweets captured ********\n",
      "******** 450 tweets captured ********\n",
      "******** 500 tweets captured ********\n",
      "******** 550 tweets captured ********\n",
      "******** 600 tweets captured ********\n",
      "******** 650 tweets captured ********\n",
      "******** 700 tweets captured ********\n",
      "******** 750 tweets captured ********\n",
      "******** 800 tweets captured ********\n",
      "******** 850 tweets captured ********\n",
      "******** 900 tweets captured ********\n",
      "******** 950 tweets captured ********\n",
      "******** 1000 tweets captured ********\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "max_tweets=1000\n",
    "text=[]\n",
    "\n",
    "# Retrieve tweets about 'The Bachelorette' using GetStreamFilter\n",
    "for line in t.GetStreamFilter(track=['thebachelorette']):\n",
    "    text.append(line)\n",
    "    if c > max_tweets:\n",
    "        break\n",
    "    c+=1\n",
    "    if c%(max_tweets*.05)==0:\n",
    "        print('********',c,'tweets captured ********')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'bachelorette.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "Clean and write tweets to file in format:\n",
    "\n",
    "USERNAME, TWEET\n",
    "\n",
    "USERNAME1, TWEET1\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "USERNAMEX, TWEETX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Saving tweets\n",
    "f = open(file_name,'w')\n",
    "for this in text:\n",
    "    try:\n",
    "        append=''\n",
    "        for char in this['text']: # Remove all new line characters from the tweets for analysis purposes\n",
    "            if char !='\\n':\n",
    "                append+=char\n",
    "        f.write(this['user']['name'].rstrip()+','+append+'\\n')\n",
    "    except:\n",
    "        pass\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting location specific tweets\n",
    "\n",
    "*This section is not part of the main objective.*\n",
    "\n",
    "Rather, this is an example of pulling tweets within a circular geographical region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting specific tweets with location and or term\n",
    "\n",
    "tms = ['a','the','with','you','when','of','from','this','and','to','that']\n",
    "g = [41.257556, -95.948950, \"35mi\"] # Location to pull from\n",
    "\n",
    "def specific_tweets(terms,geo):\n",
    "    #thesearch = t.GetSearch(geocode=geo,result_type=\"recent\",count=500,term=terms)\n",
    "    with open('location.csv','w') as f:\n",
    "        for tweet in t.GetSearch(geocode=geo,result_type=\"recent\",count=500,term=None):\n",
    "            new_tweet = ''\n",
    "            for char in tweet.text:\n",
    "                if char != '\\n' and char != ',':\n",
    "                    new_tweet+=char\n",
    "                else:\n",
    "                    new_tweet+=' '\n",
    "            f.write(tweet.user.name+','+new_tweet+'\\n')\n",
    "    return 'location.csv'\n",
    "\n",
    "\n",
    "file_name = specific_tweets(tms, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Analysis\n",
    "\n",
    "Now, using Spark, map-reduce the tweets to determine the most common words, positive words, negative words, and two word phrase.\n",
    "\n",
    "Why the Bachelorette? During the creation of this notebook, the Bachelorette was a hot topic on Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Jan,RT @BachelorBob_: Wanted to update everyone with my top three after tonight's episode. #TheBachelorette https://t.co/N7aklslNCA\",\n",
       " 'Katrina Gia,RT @enews: \"Can I pray over you before I leave?\" #TheBachelorette https://t.co/XW9a3N0E3k',\n",
       " \"Deborah Lira Hair Stylist,RT @TwitterMoments: The windmill is the star of tonight's #TheBachelorette. https://t.co/mOknNOOxsi\",\n",
       " 'NICKYJAM,RT @morganemilyg: Tyler’s parents deserve an award for how they raised him...a true gentleman. Wow i love him. #TheBachelorette',\n",
       " 'nat,RT @AlabamaHannah: when it rains, it pours. boy, get gone. #TheBachelorette']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twm = sc.textFile(file_name) # Spark RDD object\n",
    "twm.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count:\n",
      "\t thebachelorette \t 1203\n",
      "\t luke \t 532\n",
      "\t hannah \t 396\n",
      "\t tyler \t 317\n",
      "\t alabamahannah \t 279\n",
      "\t p \t 227\n",
      "\t don \t 184\n",
      "\t sex \t 171\n",
      "\t bachelorette \t 158\n",
      "\t windmill \t 156\n",
      "\t jed \t 136\n",
      "\t c \t 112\n",
      "\t finally \t 102\n",
      "\t re \t 97\n",
      "\t brettsvergara \t 92\n",
      "\t had \t 91\n",
      "\t gone \t 88\n",
      "\t boy \t 87\n",
      "\t rains \t 84\n",
      "\t pours \t 84\n",
      "\n",
      "Positive count:\n",
      "\t welcome \t 79\n",
      "\t love \t 69\n",
      "\t clarity \t 39\n",
      "\t best \t 27\n",
      "\t right \t 27\n",
      "\t fans \t 23\n",
      "\t respectful \t 23\n",
      "\t respect \t 19\n",
      "\t sexy \t 18\n",
      "\t loves \t 17\n",
      "\n",
      "Negative count:\n",
      "\t slut \t 23\n",
      "\t die \t 21\n",
      "\t hell \t 20\n",
      "\t refusing \t 20\n",
      "\t toxic \t 18\n",
      "\t steal \t 17\n",
      "\t shit \t 12\n",
      "\t bad \t 10\n",
      "\t sin \t 9\n",
      "\t savage \t 8\n",
      "\n",
      "Character count:\n",
      "\t , \t 1768\n",
      "\t : \t 1524\n",
      "\t . \t 1522\n",
      "\t  @ \t 1362\n",
      "\t :// \t 999\n",
      "\t / \t 980\n",
      "\t ’ \t 851\n",
      "\t  # \t 541\n",
      "\t … \t 374\n",
      "\t . # \t 235\n",
      "\n",
      "Two word count:\n",
      "\t pours boy \t 84\n",
      "\t finally done \t 83\n",
      "\t had sex \t 79\n",
      "\t re welcome \t 76\n",
      "\t rose ceremony \t 55\n",
      "\t Pilot Pete \t 44\n",
      "\t Jed Tyler \t 29\n",
      "\t Tyler gets \t 28\n",
      "\t pray over \t 21\n",
      "\t fantasy suites \t 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wc,ps,cc,ns,tws\n",
    "\n",
    "it = wc.word_count(twm)\n",
    "print(\"Word count:\")\n",
    "for each in it.take(20):\n",
    "    print('\\t',each[0],'\\t',each[1])\n",
    "print()\n",
    "\n",
    "ti = ps.positive_count(twm)\n",
    "print(\"Positive count:\")\n",
    "for each in ti.take(10):\n",
    "    print('\\t',each[0],'\\t',each[1])\n",
    "print()\n",
    "\n",
    "ii = ns.negative_count(twm)\n",
    "print(\"Negative count:\")\n",
    "for each in ii.take(10):\n",
    "    print('\\t',each[0],'\\t',each[1])\n",
    "print()\n",
    "\n",
    "tt = cc.char_count(twm)\n",
    "print(\"Character count:\")\n",
    "for each in tt.take(10):\n",
    "    print('\\t',each[0],'\\t',each[1])\n",
    "print()\n",
    "\n",
    "tw = tws.two_words_count(twm)\n",
    "print(\"Two word count:\")\n",
    "for each in tw.take(10):\n",
    "    print('\\t',each[0],'\\t',each[1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Conclusion*\n",
    "\n",
    "Without reading any of the 1,800+ tweets pulled, we can deduce that **this day's episode must have been about Luke, Tyler, Hannah, Jed, a windmill, Pilot Pete, and a rose ceremony**. This may be a no-duh in context of the Bachelorette, but using the same method, one could pull tweets about anything and determine the latest news on that subject by using Spark to find commonalities in each tweet.\n",
    "\n",
    "Mentioned above, there is also the possbility to analyze tweets from a specific location and deduce any kind of information desired."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
